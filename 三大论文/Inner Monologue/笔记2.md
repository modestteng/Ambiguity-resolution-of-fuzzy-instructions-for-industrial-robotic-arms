| 问题           | 答案                                           |
| -------------- | ---------------------------------------------- |
| CLIP 是什么？  | 一个把图像和文本映射到同一向量空间的预训练模型 |
| 怎么训练的？   | 对比学习：配对的拉近，不配对的推远             |
| 为什么强大？   | 4亿数据 + 开放词汇 = 超强泛化能力              |
| 对你有什么用？ | 理解模糊语言指令、定位目标物体的基础能力       |

CLIP是对比语言图像预训练，搭建了语言和这个图像识别的桥梁

CLIP 是让机器人"睁眼看世界"并"理解人话"的桥梁。



![image-20260124144707890](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124144707890.png)

相当于是加了一个是否成功的判断

idea1

路线 B（工业向）

如果没有 RL Value Function，还能不能做 SayCan？

用几何可达性

用 grasp success 预测

用规则 / simulator

👉 这会直接变成你论文的 Method Section

idea2

你的课题核心是：“那个/那里”不清楚时怎么办。

Success Detection 给你一个特别实用的思路：

消歧不一定要在执行前一次性解决，可以先做一个“安全的试探动作”，然后用 success/failure 作为证据缩小歧义。

举个工业机械臂例子：

指令：“把那个螺丝放到那里”

你不确定“那个螺丝”是哪一个
你可以先 pick 候选 A

Success=False（没抓到/抓错/抓不起来）→ 候选 A 被排除

Success=True → 继续确认“那里”的放置

这就是把“语言歧义”转成“可交互验证”的过程。

### 底层策略的位置

![image-20260124150903785](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124150903785.png)

![image-20260124153018254](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124153018254.png)

**MDETR** = **M**odulated **D**etection **T**ransfo**R**mer

中文可以理解为：**"被语言调控的检测器"**

这个MDERT更像是专门用来处理这个位置信息的一种检测器

![image-20260124153258635](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124153258635.png)

这个LLM像是一个这个CPU，相当于是一个大脑的决策层
在不同的层次里面有着不同的用法，你可以将他理解为这个大脑即可

 这个就是LLM的功能

```
用户："帮我收拾桌子"
                              │
                              ▼
                    ┌─────────────────┐
                    │      LLM        │
                    │  ┌───────────┐  │
                    │  │ 理解意图   │  │  ← 角色4：对话接口
                    │  └───────────┘  │
                    │        │        │
                    │        ▼        │
                    │  ┌───────────┐  │
                    │  │ 常识推理   │  │  ← 角色2：常识推理
                    │  └───────────┘  │
                    │        │        │
                    │        ▼        │
                    │  ┌───────────┐  │
                    │  │ 分解任务   │  │  ← 角色1：高层规划
                    │  └───────────┘  │
                    │        │        │
                    │        ▼        │
                    │  ┌───────────┐  │
                    │  │ 接受反馈   │  │  ← 角色3：内心独白
                    │  │ 调整计划   │  │
                    │  └───────────┘  │
                    └─────────────────┘
                              │
              ┌───────────────┼───────────────┐
              ▼               ▼               ▼
         视觉系统         机械臂执行        成功检测
         (MDETR)         (低层控制)       (反馈信号)
         
```

## 非常重要的三种反馈场景

![image-20260124155037591](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124155037591.png)



# 局限性：

![image-20260124160823883](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124160823883.png)



原来如此，这个prompt为什么要加上这个历史的记录，是为了抛弃已经尝试过的方法，更官方的去说，实际上他是保留了这个日志信息

![image-20260124161828643](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124161828643.png)



### 这就是他的这个历史记录的意义

![image-20260124162106090](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124162106090.png)

**所以两者是互补的：**

- **历史**：告诉 LLM "发生了什么"（因果关系）
- **当前场景**：告诉 LLM "现在是什么样"（最新状态）

可以，历史信息和当下信息的相辅相成



好一个文本区域对其，这个YOLO不能很好的根据这个语言找到这个对应的这个物体，但是这个MDETR可以，它对这个模糊语言拥有更加精准的识别能力

![image-20260124163352638](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124163352638.png)

也就是我们所说的这个语言和对象的匹配能力

![image-20260124163647679](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124163647679.png)

一个是语言和图片的对齐，一个是这个图片和图片中检测物体的对齐，只能说是这个侧重点不一样















