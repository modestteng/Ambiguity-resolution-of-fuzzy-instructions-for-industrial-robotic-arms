### SayCan 的执行循环

1. 给定用户指令 `i`
2. 枚举所有技能 π ∈ Π
   - 对每个技能：
     - LLM 打分：**这个技能语义上是不是下一步？**
     - Value Function 打分：**现在能不能成功？**
3. 相乘，选分数最高的技能
4. 执行这个技能
5. 更新环境状态
6. 把“我刚才做了什么”追加进 prompt
7. 回到第 2 步，直到输出 `done`

![image-20260123200750918](../../../../AppData/Roaming/Typora/typora-user-images/image-20260123200750918.png)

其实就是取这几个集合中的所有可行性的最大值



这个语言消歧SayCan策略，我觉得是这样理解的

1.用户给出最高指令

2.遍历机器可执行动作

3.每一个动作和最高指令的匹配评分

4.每一个动作在当前状况下的可执行性评分

5.两者相乘，求取最大值，对应的机器可执行动作，为机器的真实执行动作

6.执行动作之后S，即环境状态发生改变，需更改机器可执行动作的affordance评分

7.循环该过程

机器不是创造出来动作，而是在已经设计好的这个动作之中进行选择

![image-20260123202349307](../../../../AppData/Roaming/Typora/typora-user-images/image-20260123202349307.png)

### 好一个联合最优，好一个世界的普遍联系和永恒发展



好一个动态消歧，在执行的过程中，这个世界的状态也会不断的发生改变，也就是改变公式里面的这个S参数，世界状态，也就是这个affordance参数

![image-20260123202541849](../../../../AppData/Roaming/Typora/typora-user-images/image-20260123202541849.png)

### 好一个歧义坍缩

![image-20260123203155871](../../../../AppData/Roaming/Typora/typora-user-images/image-20260123203155871.png)



在这个SayCan中这个prompt指的是一种模版，我们把它翻译成“规划上下文”

![image-20260123212128997](../../../../AppData/Roaming/Typora/typora-user-images/image-20260123212128997.png)



#### 果然，只要理解了这个prompt是事先写好写进这个模版的，我们就理解了这个规划上下文的本质

![image-20260123212546900](../../../../AppData/Roaming/Typora/typora-user-images/image-20260123212546900.png)



![image-20260124085520168](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124085520168.png)

匹配技能指令，匹配可行性

## SayCan策略的问题有很多，不仅仅是在这个模糊指令上面，还有的是这个skill库数据爆炸的问题

![image-20260124090525532](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124090525532.png)

## 1) 一句话定义（最常用，最稳）

> **SayCan（Ahn et al., 2022）是一种技能级任务规划框架/方法**：在给定高层自然语言指令 $i$ 和技能集合 $\Pi$ 时，它用语言模型给每个技能文本 $\ell_\pi$ 打“该不该做”的分数 $p(\ell_\pi\mid i)$，用可执行性/affordance 给“能不能做成”的分数 $p(c_\pi\mid s,\ell_\pi)$，两者相乘选择下一步技能并迭代执行直到 “done”。Ahn 等 - 2022 - Do As I Can, Not…Ahn 等 - 2022 - Do As I Can, Not…

（你下次就可以说：**“SayCan 框架/方法”** 或 **“SayCan 规划算法”**）

------

## 2) 两句话讲创新点（你写 related work 够用）

> **创新点 1：把“语言合理性”和“世界可行性”解耦**：语言侧做 task-grounding（下一步是否有用），世界侧做 world-grounding（当前状态下是否能成功），并在同一概率接口中融合。Ahn 等 - 2022 - Do As I Can, Not…Ahn 等 - 2022 - Do As I Can, Not…

> **创新点 2：用“评分式 LLM + 严格响应结构”做技能选择**：不是让 LLM 自由生成计划，而是对候选技能进行评分，并把已选技能追加到上下文里做迭代规划。Ahn 等 - 2022 - Do As I Can, Not…Ahn 等 - 2022 - Do As I Can, Not…

------

## 3) 两句话讲局限（你写 critique/讨论部分够用）

> **局限 1：能力上限受限于技能库与低层技能可靠性**：只有当技能足够多且足够稳定，SayCan 的长序列规划才真正有用；不稳定或不适合长任务的技能会被弃用。 Ahn 等 - 2022 - Do As I Can, Not…Ahn 等 - 2022 - Do As I Can, Not…

> **局限 2：闭环反馈有限**：在原始设定中，系统主要通过当前决策步的 value/affordance 得到环境反馈；如果技能失败或环境改变，反馈可能不足，后续工作用 inner monologue 等方式补强闭环。 Ahn 等 - 2022 - Do As I Can, Not…Ahn 等 - 2022 - Do As I Can, Not…

------

## 4) “落地结果”一句话（可选，写实验动机时用）

> 作者在真实厨房验证了从 mock 到 real 的迁移：规划成功率下降约 3%，执行成功率下降约 14%，显示底层策略与 value/affordance 具有一定泛化能力。 Ahn 等 - 2022 - Do As I Can, Not…

------

# 5) 你课题专用版本（工业机械臂 + 模糊指令语义消歧）

如果你要把 SayCan 这套话术改成“你的论文会用的版本”，我建议这样写：

> **我们借鉴 SayCan 的技能级规划思想，将模糊高层指令转化为“候选技能集合上的排序问题”**：语言侧提供“语义上更像下一步”的评分，世界侧用工业约束/可达性/碰撞/对准成功率等构造“可执行性评分”，两者融合选择下一步动作并迭代更新，从而在执行过程中逐步消歧。 （对应 SayCan 的 $p(\ell_\pi\mid i)\times p(c_\pi\mid s,\ell_\pi)$ 结构）Ahn 等 - 2022 - Do As I Can, Not…



确实，LLM模型就是用来做这个推理的，那么不同的LLM模型在推理上面的不同体现着他们之间的优劣



### 接下来阅读inner monologue论文







