VIMA 的全称是 **VisuoMotor Attention（Agent）**，直译就是 **“视觉-运动注意力（智能体）”**：

- **VisuoMotor**：视觉（vision）+ 运动控制（motor）
- **Attention**：注意力机制（用 Transformer 的 attention 去对齐“看到的东西”和“该怎么动”）

智能体（Agent）是一个在环境中为达成目标而进行多步决策与行动的闭环系统，通常包含感知、状态表示、策略/规划与执行，并根据反馈持续调整。

![image-20260124201003885](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124201003885.png)

**为什么需要 Token？** Transformer 模型只能处理**序列**（一串东西），所以必须把输入切成一个个 token 排成队。

这个拆解的一个又一个token指的就是这个张量



位置信息（MLP encoder）（多层感知机）和这个形状特点（Vision transform encoder）信息，两方面确定这个图像的特征，我们根据这个特征将其构造成一个信息单元token

````
## 🎨 图示总结
```
        检测到一个苹果
              │
    ┌─────────┴─────────┐
    │                   │
    ▼                   ▼
┌─────────┐       ┌──────────┐
│Bounding │       │ 裁剪图像  │
│  Box    │       │          │
│[0.25,   │       │   🍎     │
│ 0.3,    │       │          │
│ 0.15,   │       └──────────┘
│ 0.12]   │             │
└─────────┘             │
    │                   ▼
    │              ┌─────────┐
    ▼              │  ViT    │
┌─────────┐        │ Encoder │
│  MLP    │        └─────────┘
│ Encoder │             │
└─────────┘             │
    │                   │
    ▼                   ▼
  [768维]            [768维]
 位置特征           外观特征
    │                   │
    └────────┬──────────┘
             │
             ▼
        ┌─────────┐
        │ Concat  │
        │    +    │
        │ Project │
        └─────────┘
             │
             ▼
        ┌─────────┐
        │ Object  │
        │  Token  │  ← 最终的物体表示！
        │ [768维] │     同时包含"在哪"和"是什么"
        └─────────┘
````



### 你之前那句话怎么修正更准确？

你说：“先检测有用物体 → 找有用 patch → transform → 处理”。

更贴近 VIMA 的说法是：

> **先检测出物体（不一定只选“有用”的，而是把场景/提示里的物体都 token 化），对每个物体：用 ViT 编码外观、用 MLP 编码框位置，然后把这些 object tokens 作为序列喂给 Transformer。**

确实如此，应当刚开始的时候检测这个所有的这个物体，然后通过多层感知机和这个图像转换器构成这个信息单元token，根据transform转换成这个张量最后根据这个LLM进行决策

这就是这个VIMA的多模态的最大革新点，可以将这个图片当做这个指挥，我们的手段本质是将其转换为这个张量的格式

![image-20260124204315423](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124204315423.png)

Prompt Tokenizer好一个tokenizer，token化，这个名字起的不错

![image-20260124210125422](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124210125422.png)

这个第三步相当于一个新的编码器，相当于将这个向量（张量）形式转换为这个机器可理解的语言，相当于一层解码，一层语言转换，我们通过一个已经训练好的模型T5进行这个转换

![image-20260124210521954](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124210521954.png)

有意思，根据这个已经规划好的上下文进行这个对应的操作（prompt）

### 4.3 两者如何配合？（重复 L 层）

```
for i in range(L层):
    # 第一步：问问指令说要干嘛
    x = CrossAttention(Q=我的记忆, KV=指令)
    
    # 第二步：回顾我之前做了什么
    x = CausalSelfAttention(Q=x, KV=x)
```

**重复多次的原因**：理解可以**层层加深**

- 第 1 层：粗略理解"要把东西放进容器"
- 第 2 层：更细理解"是苹果，放进碗"
- 第 3 层：最细理解"苹果在左边，碗在右边"

#### 好一个理解可以这个层层加深

![image-20260124210852950](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124210852950.png)

好一个根据上述指令进行这个动作编码，这就是最终的目的，就是这个进行这个最终的执行动作，根据已知的信息

## 从接受这个指令，到这个信息的具体执行的这个流程

```
用户指令: "把 [图:🍎] 放进 [图:🥣]"

═══════════════════════════════════════════════════════
Step 1: Prompt Tokenizer 切碎
═══════════════════════════════════════════════════════
"把" → [把]
"放进" → [放][进]  
[图:🍎] → [苹果token: 位置(0.2,0.3) + 红色圆形]
[图:🥣] → [碗token: 位置(0.8,0.6) + 白色凹形]

═══════════════════════════════════════════════════════
Step 2: T5 Encoder 理解
═══════════════════════════════════════════════════════
输入: [把, 苹果token, 放, 进, 碗token]
输出: 一串向量，表示"把左边的红苹果放进右边的白碗"

═══════════════════════════════════════════════════════
Step 3: Robot Controller 决策
═══════════════════════════════════════════════════════

  ┌─ 时刻 1 ──────────────────────────────────────┐
  │                                                │
  │  当前观测: 看到桌面上有苹果和碗                  │
  │       │                                        │
  │       ▼                                        │
  │  Cross-Attention: "指令说要把苹果放进碗"        │
  │       │                                        │
  │       ▼                                        │
  │  Self-Attention: "我还没做任何事"               │
  │       │                                        │
  │       ▼                                        │
  │  决策: "先去抓苹果"                             │
  └────────────────────────────────────────────────┘
                          │
                          ▼
  ┌─ 时刻 2 ──────────────────────────────────────┐
  │                                                │
  │  当前观测: 机械臂移动到了苹果上方               │
  │       │                                        │
  │       ▼                                        │
  │  Cross-Attention: "指令说要把苹果放进碗"        │
  │       │                                        │
  │       ▼                                        │
  │  Self-Attention: "我已经到了苹果上方"           │
  │       │                                        │
  │       ▼                                        │
  │  决策: "抓取，然后去碗的位置"                   │
  └────────────────────────────────────────────────┘
                          │
                          ▼
                        ...

═══════════════════════════════════════════════════════
Step 4: Action Decoder 输出
═══════════════════════════════════════════════════════
最终动作: pick(0.2, 0.3, 0°) → place(0.8, 0.6, 0°)
         "在(0.2,0.3)抓取，在(0.8,0.6)放下"
```

![image-20260124211250907](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124211250907.png)

交叉注意和这个自我注意，一个是始终注意这个历史，一个是始终注意这个指令的执行

![image-20260124211606215](../../../../AppData/Roaming/Typora/typora-user-images/image-20260124211606215.png)

## 好一个查询它，真是天才的思想，足够的富有这个创新性，具体指令——规划上下文prompt——执行——查询上下文——继续执行

**你不需要深入理解四元数的数学**，只需知道：

> 四元数是一种**更稳定**的旋转表示方法，避免了角度跳变的问题

世界的专业性质越来越强，我们有时候无需了解这个东西的具体的含义，只需要知道他的这个具体的用法即可，为了快速跟上这个时代的发展

![image-20260125085726057](../../../../AppData/Roaming/Typora/typora-user-images/image-20260125085726057.png)

这些都是具体操作的时候的动作规范，规范这个操作精度，规范这个如何操作，二维特殊欧几里得群等等，好一个动作空间，这个名字起的不错

![image-20260125095002231](../../../../AppData/Roaming/Typora/typora-user-images/image-20260125095002231.png)

特殊的算法使得这个对数据集的需求量降低了

![image-20260125095537514](../../../../AppData/Roaming/Typora/typora-user-images/image-20260125095537514.png)

真是精彩绝伦的思想，也不知道这篇文章属于什么层次

![image-20260125105227976](../../../../AppData/Roaming/Typora/typora-user-images/image-20260125105227976.png)

这个策略函数的这些字母用的还真是非常的形象

![image-20260125105652259](../../../../AppData/Roaming/Typora/typora-user-images/image-20260125105652259.png)

它总是考虑了这个需要的和当下的进行的结合

![image-20260125105840741](../../../../AppData/Roaming/Typora/typora-user-images/image-20260125105840741.png)

则个softmax激活函数还是增强了其他元素的比重，是为了防止这个过拟合吗

![image-20260125110347858](../../../../AppData/Roaming/Typora/typora-user-images/image-20260125110347858.png)

好一个详细的信息的转换，相当于是这个T5的功能是吗

![image-20260125111139114](../../../../AppData/Roaming/Typora/typora-user-images/image-20260125111139114.png)

好一个概率，负对数，总损失

![image-20260125111416215](../../../../AppData/Roaming/Typora/typora-user-images/image-20260125111416215.png)

正确的反面就是这个损失，他这一层嵌套加的真是精彩

![image-20260125112150706](../../../../AppData/Roaming/Typora/typora-user-images/image-20260125112150706.png)

有意思这个真实情况和这个实际情况之间，因为已经知道了这个最后的这个结果，因此可以直接进行计算这个损失函数

## ✅ 总结

| 公式部分                                    | 含义                   | 大白话                   |
| ------------------------------------------- | ---------------------- | ------------------------ |
| $\pi_\theta(a_t\|\mathcal{P},\mathcal{H}) $ | 模型预测正确动作的概率 | "模型觉得专家动作有多对" |
| $-\log(\cdot) $                             | 把概率转成损失         | "概率越低，惩罚越重"     |
| $\sum_{t=1}^{T} $                           | 每一步损失求和         | "整个任务的总错误"       |
| $\min_\theta $                              | 调整参数最小化损失     | "不断练习，减少错误"     |

这个就是根据损失函数调整这个参数的过程，以达到这个行为模仿的最大化

![image-20260125160150319](../../../../AppData/Roaming/Typora/typora-user-images/image-20260125160150319.png)

带上图的四种指令类型

![image-20260125160950152](../../../../AppData/Roaming/Typora/typora-user-images/image-20260125160950152.png)

好一个迁移能力和泛化学习

![image-20260125161406812](../../../../AppData/Roaming/Typora/typora-user-images/image-20260125161406812.png)

对事物进行创新，很大程度上就是模仿人的过程，确实这就是人工智能的核心之一，模仿人的特征，那么我们以后在做这个课题的时候，就要想想人，或者说自己是如何做的，然后自己的行为如何在这个计算机上进行这个实现

![image-20260125161731999](../../../../AppData/Roaming/Typora/typora-user-images/image-20260125161731999.png)

这张图就很好的解释了这个，交叉注意力机制corss-Attention，会一直回头看这个prompt

### 为什么 VIMA 更鲁棒？

论文给出的解释：

> **Object-centric representation（以物体为中心的表示）** 比直接从像素学习更不容易过拟合。

好一个更不容易进行这个过拟合，object-centric不容易将这个背景的无用信息给引入

![image-20260125164317495](../../../../AppData/Roaming/Typora/typora-user-images/image-20260125164317495.png)

这个概念也算是非常简单

只是有时候没有人用人话进行这个讲解

这里的 **baseline（基线方法/对照组）**，你可以把它理解成：

> **为了证明 VIMA 的设计“确实更好”，作者必须找一些“合理、常见、能跑通”的替代方案当对照组**。
>  这些对照组就是 baseline：**同样的任务、同样的数据、尽量相同的训练设置，只改某个关键设计点**，看性能差多少。

所以 **baseline 不是“随便找的模型”**，而是“用来对比、证明贡献”的参照物。

好一个基准模型，这个做实验这一块，让我回想起了这个高中的生物和这个物理，可惜我已经发生了很大的改变，已经不是过去的自己了，这些东西，随着我学习能力和认知的提升，已经，不可用过去的眼光进行这个衡量

![image-20260125170534688](../../../../AppData/Roaming/Typora/typora-user-images/image-20260125170534688.png)

对于纯语音指令，可以通过这个语音本身所含有的信息构造多模态

**图片不一定要用户提供**——可以由系统的摄像头自动捕获

工人说"把那个螺丝拧紧"→ 系统用 Grounding DINO 检测"螺丝" → 自动生成图片 token



