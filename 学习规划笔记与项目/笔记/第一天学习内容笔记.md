![image-20260126201641601](../../../../AppData/Roaming/Typora/typora-user-images/image-20260126201641601.png)

好一个最终目的是这个视觉落地，认识类别和对应哪一个，这些都是最终能够将这个视觉落地的体现，这两个联合起来是真的牛逼呀

## 1️⃣ GroundingDINO（接地恐龙 🦖）

### 名字拆解

- **Grounding** = 落地（语言→图像坐标）
- **DINO** = **D**ETR with **I**mproved de**N**oising anch**O**r boxes（一种强大的目标检测架构）

这个DIDO和这个YOLO刚好一个是闭集一个是开放集

```
"把那个螺丝拧到那里"
         │
         ▼
┌─────────────────────────────────┐
│  GroundingDINO（候选生成器）      │
│  ├── "螺丝" → [螺丝A, 螺丝B, 螺丝C]  │
│  └── "那里" → [孔位1, 孔位2]       │
└─────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────┐
│  消歧模块（你课题的核心）          │
│  ├── 结合上下文 → 螺丝B            │
│  └── 结合任务逻辑 → 孔位1          │
└─────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────┐
│  (可选) SAM 精细分割              │
│  └── 螺丝B的精确轮廓 → 抓取点规划   │
└─────────────────────────────────┘
         │
         ▼
    机械臂执行抓取和拧入动作
```

![image-20260126204433375](../../../../AppData/Roaming/Typora/typora-user-images/image-20260126204433375.png)

可以层层递进，越来越具有这个普遍性

![image-20260126211934588](../../../../AppData/Roaming/Typora/typora-user-images/image-20260126211934588.png)

自我注意力就是将这个建立文本之间的相互关系



### 一些概念的辨析

## 1) RefCOCO / RefCOCO+ / RefCOCOg 是什么？

它们都是 **“指代表达数据集”**（Referring Expression Dataset）：
 每条数据 = **一张图 + 一句话描述 + 这句话指向的那个目标框**。

你可以理解成：在图里做“找茬”，但线索是自然语言。

### RefCOCO（允许位置词）

- 特点：描述里**可以出现位置关系词**，比如 *left/right/front/behind*
- 例子：`boy on left`（左边的男孩）
- 难度直觉：**相对更容易**，因为“左边”这类词很强的定位线索。

> 对工业场景：像“左边那颗螺丝”“靠近夹爪的那个零件”。

### RefCOCO+（禁止位置词）

- 特点：描述里**不能用位置词**（不让你靠“左边/右边”这种捷径）
- 例子：`a kid in blue shirt`（穿蓝衬衫的小孩）
- 难度直觉：**更难**，模型必须靠外观属性（颜色、材质、形状、部件）来选对。

> 对工业场景：像“红色螺丝”“带二维码标签的盒子”，不能说“左边那个”。

### RefCOCOg（描述更长、更啰嗦）

- 特点：句子通常**更长、信息更丰富**，可能包含多个属性和关系
- 例子：`the boy wearing a black helmet standing at bat`（戴黑头盔、站在球棒位置的男孩）
- 难度直觉：不是“更难或更易”固定的——
   线索多了有时更好找，但也更考验模型**把长句拆成关键约束**。

> 对工业场景：像“靠近夹具、带黑色橡胶圈、旁边有一根气管的那个接头”。

------

## 2) val / testA / testB 是什么？

它们都是**数据集切分（split）**，用来公平评估：

- **val（验证集）**：训练时用来调参、看趋势（不参与最终“成绩”）
- **test（测试集）**：最终报告成绩用

RefCOCO 里常见：

- **testA**：偏“人”相关的样本更多（你截图里也写了人物场景）
- **testB**：偏“物体”相关的样本更多

> 对你更关键的是 testB 的意义：更贴近工业“物体/零件”场景。

------

## 3) Zero-shot 是什么？

**中文：零样本 / 零训练迁移**
 意思是：**不在这个数据集上训练**（比如不在 RefCOCO 上训练），直接拿模型去做 RefCOCO 测试。

直觉：

- 像你从没刷过“指代题”，只刷过“检测题”，现在直接去做“根据一句话找那个物体”的题——一般会吃亏。
- 这能测模型的“泛化能力”：到底是不是“见过才会”。

------

## 4) Fine-tune 是什么？

**中文：微调**
 在目标数据集（比如 RefCOCO）上**继续训练一小段**，让模型适应这种任务的“题型”。

直觉：

- 像做题前先刷一套同类型真题，模型就学会：
   “这类句子里，颜色/位置/关系词分别怎么用来选目标”。

------

## 5) SOTA 是什么？

**中文：当前最好水平（State Of The Art）**
 就是“在这个榜单/任务上目前最强的结果”。

------

## 6) 这些概念和你“模糊指令消歧”怎么挂钩？

你要做的“消歧”通常分两步：

1. **先 grounding 出候选**（可能不止一个）：这更像 RefCOCO/RefCOCO+ 的能力
2. **再通过追问/多模态信息选对那个**（你的研究重点）

所以你截图里“zero-shot 一般、fine-tune 提升巨大”的含义很现实：

> **只靠通用检测训练，模型对“细粒度指代”（那个/哪里/靠近谁）会不够稳；有指代数据或类似监督就会显著变强。**

![image-20260130151359934](../../../../AppData/Roaming/Typora/typora-user-images/image-20260130151359934.png)

互相交叉融合，是一种看着这个prompt的逐渐的融合，那么在此基础上我们使用这个cross-attention就变得很合理了

有意思，这个cross其实是这个跨模态的意识

好一个编码器和解码器encoder和decoder

![image-20260130153700918](../../../../AppData/Roaming/Typora/typora-user-images/image-20260130153700918.png)

这样确实可以保留更多的信息，而且计算量和正确率的综合性最好，每一个图像最像文本信息的图像位置的全部标出，真是精彩

![image-20260130153841970](../../../../AppData/Roaming/Typora/typora-user-images/image-20260130153841970.png)

由语言引导的这个查询选择

### 你现在只要牢牢记住这句话就够了

**语言引导的 query 选择 = 用文本给每个图像位置打分，挑出最可能相关的 900 个位置，减少计算，同时尽量不漏掉目标。**

![image-20260130154538690](../../../../AppData/Roaming/Typora/typora-user-images/image-20260130154538690.png)

确实，如果部件和部件连接在一起，它又很难理解了

![image-20260130155328620](../../../../AppData/Roaming/Typora/typora-user-images/image-20260130155328620.png)

从数量上来看，两者还有区别，我觉得这个理解不精确，这个指代表达理解，应该是这个更加根据文本精确的定位

召回，落地阶段grounding
