



# 论文：

![image-20260120164906784](../../AppData/Roaming/Typora/typora-user-images/image-20260120164906784.png)

```

输入层：  State（环境状态/图像） + Instruction（语言指令）
           ↓                        ↓
编码层：  Vision Encoder          Language Encoder
           （把图像变成向量）        （把文字变成向量）
                    ↘              ↙
融合层：            Action Decoder
                  （融合视觉+语言，决定做什么）
                           ↓
输出层：                 Action（机械臂动作）
```

![image-20260120155601888](../../AppData/Roaming/Typora/typora-user-images/image-20260120155601888.png)



# 课题聚焦：Control Policies → Multimodal Instructions



### 九大挑战总结表

| 编号 | 挑战               | 核心问题                 | 当前不足                                                     |
| :--: | ------------------ | ------------------------ | ------------------------------------------------------------ |
|  a   | **安全优先**       | 机器人直接与物理世界交互 | 缺乏安全护栏、风险评估框架、人机交互协议                     |
|  b   | **数据集与基准**   | 评估体系不完善           | 缺乏覆盖多技能、多物体、多环境的综合基准；指标单一（只有成功率） |
|  c   | **基础模型与泛化** | 机器人的"GPT时刻"未到来  | 泛化能力远不如NLP领域的LLM                                   |
|  d   | **多模态**         | 如何融合多种感知         | 嵌入对齐方式是否足够？音频、触觉、注视等模态未充分利用       |
|  e   | **长程任务框架**   | 复杂任务需要多步骤       | 分层框架增加复杂度；端到端方法不成熟                         |
|  f   | **实时响应**       | 机器人需要快速决策       | 大模型推理慢，速度与能力之间存在权衡                         |
|  g   | **多智能体系统**   | 多机器人协作             | 通信、调度、异构性问题                                       |
|  h   | **伦理与社会影响** | 社会责任                 | 隐私、就业、偏见、社会规范                                   |
|  i   | **应用场景**       | 落地部署                 | 医疗、农业等领域有特殊需求                                   |

------

### 与你课题相关的三大机会点

#### 🎯 机会1：多模态融合中的"语言歧义"问题（来自挑战d）

**原文要点**：

> 当前方法如ImageBind、LanguageBind将不同模态对齐到统一嵌入空间，但仅靠嵌入是否足够仍有争议。

**研究空白**：

- 现有多模态融合假设语言输入是**清晰明确**的
- 没有考虑语言本身的**歧义性**

**你可以做的**：

```
当用户说"把那个东西放过去"时：
- 现有方法：直接把这句话编码成向量
- 你的方法：先消歧（确定"那个"指什么，"过去"指哪里），再编码
```

------

#### 🎯 机会2：基准测试的细粒度评估（来自挑战b）

**原文要点**：

> 需要超越"成功率"的指标，对VLA模型进行细粒度诊断。

**研究空白**：

- 现有基准只评估"任务成功/失败"
- 没有专门评估**语言理解能力**，特别是**模糊指令理解能力**

**你可以做的**：

```
构建一个"模糊指令理解"基准数据集：
- 包含不同程度的模糊表达（轻度/中度/重度）
- 评估模型的消歧准确率
- 这本身就是一个贡献点！
```

------

#### 🎯 机会3：安全性中的指令误解问题（来自挑战a）

**原文要点**：

> VLA决策过程的可解释性对于错误诊断至关重要。

**研究空白**：

- 如果机器人**误解**了用户指令，可能导致危险操作
- 现有研究没有关注"指令误解"这一安全隐患

**你可以做的**：

```
你的消歧模块可以：
1. 检测指令是否模糊
2. 如果模糊，主动向用户确认（"您是指红色的杯子吗？"）
3. 降低因误解导致的安全风险

这既是技术贡献，也是安全贡献！
```

------

### 第VII章 结论 —— 翻译

> 视觉-语言-动作模型在使具身智能体与物理世界交互、执行用户指令方面具有巨大潜力。本文是首篇同时综述大型VLA和通用VLA的调研。我们的分类体系从高层次概述了三条主要研究线：关键组件、控制策略和任务规划器。我们详细分析并比较了它们的技术细节，包括模型架构、训练策略和各个模块。此外，我们还介绍了训练和评估VLA的重要资源，如数据集、仿真器和基准测试。我们希望这篇综述能够捕捉具身AI快速发展的态势，并启发未来的研究。

------

### 你的论文可以这样定位

基于以上分析，你的论文定位可以是：

```
┌─────────────────────────────────────────────────────────┐
│                                                         │
│   填补VLA研究中"语言理解"环节的空白：                    │
│                                                         │
│   现有工作假设：用户指令清晰明确                         │
│        ↓                                                │
│   实际情况：用户指令常常模糊（"把那个放过去"）           │
│        ↓                                                │
│   你的贡献：提出模糊指令消歧方法                         │
│        ↓                                                │
│   价值：提升VLA的鲁棒性和安全性                          │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

------

### 具体可做的工作方向

| 方向                                   | 难度 | 创新点       | 工作量 |
| -------------------------------------- | :--: | ------------ | :----: |
| 构建模糊指令数据集                     |  ★★  | 填补数据空白 |   中   |
| 设计消歧模块（接在Language Encoder后） | ★★★  | 方法创新     |  中高  |
| 结合视觉信息的消歧（Visual Grounding） | ★★★  | 多模态消歧   |   高   |
| 主动询问机制（检测到模糊时向用户确认） |  ★★  | 交互创新     |   中   |







# 论文定位：

```
┌─────────────────────────────────────────────────────────┐
│                                                         │
│   填补VLA研究中"语言理解"环节的空白：                    │
│                                                         │
│   现有工作假设：用户指令清晰明确                         │
│        ↓                                                │
│   实际情况：用户指令常常模糊（"把那个放过去"）           │
│        ↓                                                │
│   你的贡献：提出模糊指令消歧方法                         │
│        ↓                                                │
│   价值：提升VLA的鲁棒性和安全性                          │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

接下来就是找到现有的语义的问题，并抽取细小的点进行优化

```
论文：A Survey on Vision-Language-Action Models for Embodied AI

一句话总结：
    VLA是让机器人能"听懂指令、看懂环境、执行动作"的多模态模型

核心分类：
    1. 组件层 - VLA的基础模块
    2. 控制策略层 - 视觉+语言→动作（我的课题在这里）
    3. 任务规划层 - 分解复杂任务

关键工作：
    - RT-2：首次提出VLA概念
    - CLIP：视觉-语言对齐基础
    - OpenVLA：开源VLA

与我课题的关系：
    现有VLA假设语言指令清晰，但实际常常模糊
    → 我的工作：在Language Encoder之后加入消歧模块

待读论文：
    - RT-2
    - CLIP  
    - CLIPort
```







# 架构

```
用户指令: "Clean up the room"
          ↓
┌─────────────────────────────┐
│  High-level Task Planner    │ ← LLM/VLM（理解语义，分解任务）
│  输出：子任务序列            │
│  1. Pick up the toy car     │
│  2. Move to the coffee table│
│  3. Put near the camera     │
└─────────────────────────────┘
          ↓
┌─────────────────────────────┐
│  Low-level Control Policy   │ ← VLA（执行具体动作）
│  输出：机械臂的位移和旋转    │
└─────────────────────────────┘
```



# 模型

| 模型名称           | 核心思想            | 语言怎么和视觉融合         | 数据集         | 你要关注什么                       |
| ------------------ | ------------------- | -------------------------- | -------------- | ---------------------------------- |
| **CLIPort** (2021) | CLIP + Transporter  | 语言提取语义，视觉提取空间 | Ravens         | **最早的language-conditioned抓取** |
| **BC-Z** (2021)    | FiLM层融合          | 语言embedding调制视觉特征  | 自建数据集     | **零样本泛化**                     |
| **RT-1** (2022)    | Transformer控制策略 | FiLM + Transformer         | Fractal (130K) | **大规模数据的重要性**             |
| **RT-2** (2023)    | 大模型做控制策略    | VLM直接输出动作            | Fractal + VQA  | **首次提出VLA这个术语**            |
| **OpenVLA** (2024) | 开源版RT-2          | DINOv2 + SigLIP + LLM      | OXE, DROID     | **开源！你可以跑实验**             |



#### CLIP视觉和语言的桥梁Contrastive Language-Image Pre-training



### 好一个活的张量转换

#### 如果是 Transformer 的转换（加权注意力）：

它在转换成张量之前，会先“左顾右盼”，看看这句话里其他词是谁。

- **场景 1：“我爱吃红色的苹果。”** Transformer 看到“吃”、“红色”，它会把注意力集中在水果属性上。 最后生成的张量可能是：`[0.9 (水果味), 0.1 (科技感), 0.8 (食物)]`。
- **场景 2：“苹果发布了新手机。”** Transformer 看到“发布”、“手机”，它会把注意力集中在科技公司属性上。 最后生成的张量可能是：`[0.1 (水果味), 0.9 (科技感), 0.2 (食物)]`。

**这就是 Transformer 的核心魔力：它生成的张量，是活的，是包含语境意义的。**



![image-20260121110052753](../../AppData/Roaming/Typora/typora-user-images/image-20260121110052753.png)

不错的解释





- **BC-Z (上一代思路)：** 像个**翻译官**。左耳听指令（语言张量），右眼看环境（视觉张量），大脑拼命想把这两者结合起来，算出一个动作。
- **RT-1 (新一代思路)：** 像个**打字员**。它根本不区分什么是图、什么是字、什么是动作。对它来说，所有东西都是一长串“单词”。**做动作，就是把这篇“文章”续写下去。**

**一句话解释 RT-1：** 它把“物理世界的动作”强行数字化成了“文本符号”，让机器人只要会“做填空题”，就能学会干活。



**RT-2：** 把互联网常识装进了机器人的身体。**（懂知识，会推理）**

RT-2 让机器人第一次拥有了**“常识”**。你不再需要像写代码一样给它发指令，你可以像对人说话一样对它说：“把那个能用来砸钉子的东西给我”（它会把石头递给你，而不是把海绵递给你）

将整个互联网作为这个数据集，并且有较高的推理能力



OpenVLA是一个开源的模型，类似于RT-2，只不过他是开源的，而且勉强在个体用户的电脑上面能够跑的动



**FiLM (Feature-wise Linear Modulation)** 听起来是个很复杂的数学名词，但如果用通俗的语言解释，它就是 **AI 的“智能滤镜”** 或者 **“偏见植入器”**。

它的核心作用是：**让“语言”去控制“视觉”，告诉眼睛该关注什么。**

FiLM 的全称是“特征层面的线性调制”。别被吓到了，其实就是初中数学里的 **$y = ax + b$**。

## 区分两者的概念

**Grounding (落地/接地)：**

- **解决的问题：** **“哪个是刀？”**
- **结果：** 机器人用红框把刀圈出来。
- **潜台词：** 我认出它的名字了。

**Affordance (可供性)：**

- **解决的问题：** **“刀该抓哪儿？”**
- **结果：** 机器人标记出“刀柄”是可以抓的，“刀刃”是不能抓的（或者 Affordance 是用来“切”的，而不是用来“握”的）。
- **潜台词：** 我知道怎么用它了







![image-20260123165720307](../../../AppData/Roaming/Typora/typora-user-images/image-20260123165720307.png)



显然code-based是我们所需要的，他将这个指令通过LLM转换为这个代码，可调式可追溯，难以有歧义。相较于这个依然是语音的结果。

**LLM 就是：** 一个读完了整个互联网数据的**数学模型**。 它不理解什么是真理，什么是谎言。 它只知道：**“根据我对人类语言规律的统计，当你说了这句话之后，接下一句什么话看起来最像人话。”**

真是精彩

![image-20260123174648584](../../../AppData/Roaming/Typora/typora-user-images/image-20260123174648584.png)

好一个多模态指令，这个消歧的手段可以直接参入图片用来多模态消歧



inner monologue模型在这个saycan策略的基础上增加了这个反馈模型，将这个情况实时反馈给这个LLM，并重新进行这个策略规划

![image-20260123175814556](../../../AppData/Roaming/Typora/typora-user-images/image-20260123175814556.png)



# 三个策略的演变：

### 1）先用 SayCan 思想：双重验证（像“主任+安全员”）

- **Task grounding（主任）**：LLM 先给几个“它认为你可能想做的事”
   例如：放到左边托盘/放到右边工位/放到传送带入口……
- **World grounding（安全员）**：再用环境模型/可达性/抓取可行性/碰撞检测筛掉不可能的
   例如：右边被挡住不可达、左边可达但要先移开障碍物……

👉 启发是怎么来的？
 因为论文告诉你：**“说得通”不代表“做得到”**，所以你自然想到：模糊消歧也别只靠语言，要让“物理可行性”参与投票。

------

### 2）再用 Inner Monologue：闭环反馈（像“边干边问”）

你不一定要在开始就把所有歧义解决完。可以这样做：

- 先执行**低风险动作**（比如移动到两个候选物体都能看到的位置）
- 视觉更清楚后，再判断“哪个更像你说的那个”
- 仍不确定，就在执行中提问：
   “你指的是靠近夹具的这个吗？”（屏幕高亮）

👉 启发是怎么来的？
 论文强调：执行过程中不断拿反馈更新理解。
 所以你得到：**消歧可以是一个过程，而不是一次问答**。

------

### 3）最后用 VIMA：多模态提示（像“你直接指给我看”）

如果仍然不确定，就别让模型瞎猜了，让用户补信息：

- 用户在监控画面上**点一下目标**
- 或发一张“目标零件长什么样”的参考图
- 或给一个简短示范（更高阶）

👉 启发是怎么来的？
 因为论文指出：有些任务信息“用嘴说不清”，那就用图/示范来“更明确地定义任务”。















